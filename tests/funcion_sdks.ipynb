{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具调试 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要包导入\n",
    "\n",
    "from IPython.display import display, Markdown, Latex, JSON, Image, YouTubeVideo, Markdown, HTML, SVG, JSON, Pretty\n",
    "from rich import print as rprint\n",
    "from rich import print_json\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os \n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local.env file\n",
    "\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/guchen/repo/CyanoManus\n"
     ]
    }
   ],
   "source": [
    "# change pwd \n",
    "import sys \n",
    "from pathlib import Path\n",
    "cwd_dir = Path(os.getcwd())\n",
    "root_dir = cwd_dir.parent\n",
    "sys.path.append(str(root_dir))\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `wikipedia` 包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDFDEAL (doc2x SDK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfdeal import Doc2X\n",
    "\n",
    "client = Doc2X(\n",
    "    apikey=os.getenv(\"DOC2X_API_KEY\"),\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "\n",
    "success, failed, flag = client.pdf2file(\n",
    "    pdf_file=\"toolrl.pdf\",\n",
    "    output_path=\"./output\",\n",
    "    output_format=\"docx\",\n",
    ")\n",
    "\n",
    "print(success)\n",
    "print(failed)\n",
    "print(flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wolframe Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = \"countries with the largest child population\"\n",
    "#query = \"integrate x^2 sin^3 x dx\"\n",
    "query = \"boiling point of mercury\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arxiv \n",
    "\n",
    "https://pypi.org/project/arxiv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv \n",
    "from arxiv import ArxivError, UnexpectedEmptyPageError, HTTPError \n",
    "\n",
    "client = arxiv.Client(\n",
    "    page_size=100, # < 2000  \n",
    "    delay_seconds=3.0,\n",
    "    num_retries=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search = arxiv.Search(\n",
    "    query = \"jupiter\",  # This should be unencoded. Use `au:del_maestro AND ti:checkerboard`, not`au:del_maestro+AND+ti:checkerboard`.\n",
    "    id_list=None,\n",
    "    max_results = 10,\n",
    "    sort_by = arxiv.SortCriterion.Relevance, # : SortCriterion  Option[.LastUpdatedDate, .Relevance, .SubmittedDate]\n",
    "    sort_order = arxiv.SortOrder.Descending  # Optional: [.Ascending, .Descending]\n",
    ")\n",
    "\n",
    "try: \n",
    "    results = client.results(search)\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results: \n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "entry_id: http://arxiv.org/abs/2503.16419v3\n",
      "updated: 2025-04-23 17:46:54+00:00\n",
      "published: 2025-03-20 17:59:38+00:00\n",
      "title: Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models\n",
      "authors: Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, Xia Hu\n",
      "summary: Large Language Models (LLMs) have demonstrated remarkable capabilities in\n",
      "complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\n",
      "OpenAI o1 and DeepSeek-R1, have further improved performance in System-2\n",
      "reasoning domains like mathematics and programming by harnessing supervised\n",
      "fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\n",
      "Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\n",
      "improve performance, they also introduce significant computational overhead due\n",
      "to verbose and redundant outputs, known as the \"overthinking phenomenon\". In\n",
      "this paper, we provide the first structured survey to systematically\n",
      "investigate and explore the current progress toward achieving efficient\n",
      "reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\n",
      "categorize existing works into several key directions: (1) model-based\n",
      "efficient reasoning, which considers optimizing full-length reasoning models\n",
      "into more concise reasoning models or directly training efficient reasoning\n",
      "models; (2) reasoning output-based efficient reasoning, which aims to\n",
      "dynamically reduce reasoning steps and length during inference; (3) input\n",
      "prompts-based efficient reasoning, which seeks to enhance reasoning efficiency\n",
      "based on input prompt properties such as difficulty or length control.\n",
      "Additionally, we introduce the use of efficient data for training reasoning\n",
      "models, explore the reasoning capabilities of small language models, and\n",
      "discuss evaluation methods and benchmarking.\n",
      "comment: Project Website:\n",
      "  https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs\n",
      "journal_ref: None\n",
      "doi: None\n",
      "primary_category: cs.CL\n",
      "categories: ['cs.CL']\n",
      "links: [arxiv.Result.Link('http://arxiv.org/abs/2503.16419v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2503.16419v3', title='pdf', rel='related', content_type=None)]\n",
      "pdf_url: http://arxiv.org/pdf/2503.16419v3\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "entry_id: http://arxiv.org/abs/2504.16877v1\n",
      "updated: 2025-04-23 16:54:16+00:00\n",
      "published: 2025-04-23 16:54:16+00:00\n",
      "title: Context-Enhanced Vulnerability Detection Based on Large Language Model\n",
      "authors: Yixin Yang, Bowen Xu, Xiang Gao, Hailong Sun\n",
      "summary: Vulnerability detection is a critical aspect of software security. Accurate\n",
      "detection is essential to prevent potential security breaches and protect\n",
      "software systems from malicious attacks. Recently, vulnerability detection\n",
      "methods leveraging deep learning and large language models (LLMs) have garnered\n",
      "increasing attention. However, existing approaches often focus on analyzing\n",
      "individual files or functions, which limits their ability to gather sufficient\n",
      "contextual information. Analyzing entire repositories to gather context\n",
      "introduces significant noise and computational overhead. To address these\n",
      "challenges, we propose a context-enhanced vulnerability detection approach that\n",
      "combines program analysis with LLMs. Specifically, we use program analysis to\n",
      "extract contextual information at various levels of abstraction, thereby\n",
      "filtering out irrelevant noise. The abstracted context along with source code\n",
      "are provided to LLM for vulnerability detection. We investigate how different\n",
      "levels of contextual granularity improve LLM-based vulnerability detection\n",
      "performance. Our goal is to strike a balance between providing sufficient\n",
      "detail to accurately capture vulnerabilities and minimizing unnecessary\n",
      "complexity that could hinder model performance. Based on an extensive study\n",
      "using GPT-4, DeepSeek, and CodeLLaMA with various prompting strategies, our key\n",
      "findings includes: (1) incorporating abstracted context significantly enhances\n",
      "vulnerability detection effectiveness; (2) different models benefit from\n",
      "distinct levels of abstraction depending on their code understanding\n",
      "capabilities; and (3) capturing program behavior through program analysis for\n",
      "general LLM-based code analysis tasks can be a direction that requires further\n",
      "attention.\n",
      "comment: None\n",
      "journal_ref: None\n",
      "doi: None\n",
      "primary_category: cs.SE\n",
      "categories: ['cs.SE']\n",
      "links: [arxiv.Result.Link('http://arxiv.org/abs/2504.16877v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2504.16877v1', title='pdf', rel='related', content_type=None)]\n",
      "pdf_url: http://arxiv.org/pdf/2504.16877v1\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "entry_id: http://arxiv.org/abs/2504.16833v1\n",
      "updated: 2025-04-23 15:52:50+00:00\n",
      "published: 2025-04-23 15:52:50+00:00\n",
      "title: LRASGen: LLM-based RESTful API Specification Generation\n",
      "authors: Sida Deng, Rubing Huang, Man Zhang, Chenhui Cui, Dave Towey, Rongcun Wang\n",
      "summary: REpresentation State Transfer (REST) is an architectural style for designing\n",
      "web applications that enable scalable, stateless communication between clients\n",
      "and servers via common HTTP techniques. Web APIs that employ the REST style are\n",
      "known as RESTful (or REST) APIs. When using or testing a RESTful API,\n",
      "developers may need to employ its specification, which is often defined by\n",
      "open-source standards such as the OpenAPI Specification (OAS). However, it can\n",
      "be very time-consuming and error-prone to write and update these\n",
      "specifications, which may negatively impact the use of RESTful APIs, especially\n",
      "when the software requirements change. Many tools and methods have been\n",
      "proposed to solve this problem, such as Respector and Swagger Core. OAS\n",
      "generation can be regarded as a common text-generation task that creates a\n",
      "formal description of API endpoints derived from the source code. A potential\n",
      "solution for this may involve using Large Language Models (LLMs), which have\n",
      "strong capabilities in both code understanding and text generation. Motivated\n",
      "by this, we propose a novel approach for generating the OASs of RESTful APIs\n",
      "using LLMs: LLM-based RESTful API-Specification Generation (LRASGen). To the\n",
      "best of our knowledge, this is the first use of LLMs and API source code to\n",
      "generate OASs for RESTful APIs. Compared with existing tools and methods,\n",
      "LRASGen can generate the OASs, even when the implementation is incomplete (with\n",
      "partial code, and/or missing annotations/comments, etc.). To evaluate the\n",
      "LRASGen performance, we conducted a series of empirical studies on 20\n",
      "real-world RESTful APIs. The results show that two LLMs (GPT-4o mini and\n",
      "DeepSeek V3) can both support LARSGen to generate accurate specifications, and\n",
      "LRASGen-generated specifications cover an average of 48.85% more missed\n",
      "entities than the developer-provided specifications.\n",
      "comment: None\n",
      "journal_ref: None\n",
      "doi: None\n",
      "primary_category: cs.SE\n",
      "categories: ['cs.SE']\n",
      "links: [arxiv.Result.Link('http://arxiv.org/abs/2504.16833v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2504.16833v1', title='pdf', rel='related', content_type=None)]\n",
      "pdf_url: http://arxiv.org/pdf/2504.16833v1\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for item in client.results(\n",
    "    arxiv.Search(\n",
    "        query=\"deepseek\",\n",
    "        max_results=3,\n",
    "        sort_by=arxiv.SortCriterion.LastUpdatedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "        )\n",
    "    ):\n",
    "    \n",
    "    print(\"-\"*50)\n",
    "    print(f\"entry_id: {item.entry_id}\") # A url of the form `https://arxiv.org/abs/{id}\n",
    "    print(f\"updated: {item.updated}\") # : class 'datetime.datetime , When the result was last updated.\n",
    "    print(f\"published: {item.published}\") # : class 'datetime.datetime, When the result was originally published.\n",
    "    print(f\"title: {item.title}\") # The title of the result.\n",
    "    \n",
    "    # 将作者名字拼接成一个字符串，跳过空值\n",
    "    author_names = \", \".join([author.name for author in item.authors if author.name is not None])\n",
    "    print(f\"authors: {author_names}\")\n",
    "    \n",
    "    \n",
    "    print(f\"summary: {item.summary}\") # The result abstract.\n",
    "    print(f\"comment: {item.comment}\") # The authors' comment if present\n",
    "    print(f\"journal_ref: {item.journal_ref}\") # A journal reference if present.\n",
    "    print(f\"doi: {item.doi}\") # A URL for the resolved DOI to an external resource if present.\n",
    "    \n",
    "\n",
    "    print(f\"primary_category: {item.primary_category}\") # :str, The result's primary arXiv category. See [arXiv: Category Taxonomy](https://arxiv.org/category_taxonomy).\n",
    "    print(f\"categories: {item.categories}\") # :list[str], All of the result's categories. See [arXiv: Category Taxonomy](https://arxiv.org/category_taxonomy)\n",
    "    \n",
    "    print(f\"links: {item.links}\") # : list[arxiv.Result.Link], Up to three URLs associated with this result.\n",
    "    \n",
    "    \n",
    "    print(f\"pdf_url: {item.pdf_url}\") # The URL of a PDF version of this result if present among links\n",
    "    #print(f\"_raw: {item._raw}\") # The raw feedparser result object if this Result was constructed with Result._from_feed_entry.\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    item.download_pdf()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search specific paper with id \n",
    "\n",
    "paper = next(arxiv.Client().results(arxiv.Search(id_list=[\"2503.16419v3\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./2503.16419v3.Stop_Overthinking__A_Survey_on_Efficient_Reasoning_for_Large_Language_Models.pdf'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper = next(arxiv.Client().results(arxiv.Search(id_list=[\"1605.08386v1\"])))\n",
    "# Download the PDF to the PWD with a default filename.\n",
    "paper.download_pdf()\n",
    "# Download the PDF to the PWD with a custom filename.\n",
    "paper.download_pdf(filename=\"downloaded-paper.pdf\")\n",
    "# Download the PDF to a specified directory with a custom filename.\n",
    "paper.download_pdf(dirpath=\"./mydir\", filename=\"downloaded-paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the paper with ID \"1605.08386v1\"\n",
    "search_by_id = arxiv.Search(id_list=[\"1605.08386v1\"])\n",
    "# Reuse client to fetch the paper, then print its title.\n",
    "first_result = next(client.results(search))\n",
    "print(first_result.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download a PDF of the paper with ID \"1605.08386v1,\" run a Search and then use Result.download_pdf():\n",
    "\n",
    "paper = next(arxiv.Client().results(arxiv.Search(id_list=[\"1605.08386v1\"])))\n",
    "# Download the PDF to the PWD with a default filename.\n",
    "paper.download_pdf()\n",
    "# Download the PDF to the PWD with a custom filename.\n",
    "paper.download_pdf(filename=\"downloaded-paper.pdf\")\n",
    "# Download the PDF to a specified directory with a custom filename.\n",
    "paper.download_pdf(dirpath=\"./mydir\", filename=\"downloaded-paper.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
